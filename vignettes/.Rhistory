sigmoidFun <- function(x){
1/(1+exp(-x))
}
sigmoidDerivate <- function(x){
sigmoidFun(x) *(1-sigmoidFun(x))
}
reluFun <- function(x){
max(0,x)
}
reluDerivate <- function(x){
as.numeric(x>0)
}
tanhDerivate <- function(x){
1 - tanh(x)^2
}
# Construct neural network
myNeuralNet <- function(layers,activationFun){
# choose activation function
if(activationFun == 'tanh'){
activation<<- tanh
activationDerivate <<- tanhDerivate
}else if(activationFun == 'sigmoid'){
activation <<- sigmoidFun
activationDerivate <<- sigmoidDerivate
}else if(activationFun == 'relu'){
activation <<- reluFun
activationDerivate <<- reluDerivate
}else {
return('activation is not support,please choose tanh,relu or sigmoid')
}
initialWeights <- list()
length(initialWeights) <- length(layers)-1
for (i in 1:(length(layers)-2)) {
rows <- layers[i]+1
cols <- layers[i+1]+1
initialWeights[[i]]<- matrix(nrow = rows,ncol = cols,rnorm(rows*cols))  # Initialize Weights
}
lastRow <- layers[length(initialWeights)]+1
lastCol <- layers[length(initialWeights)+1]
initialWeights[[length(initialWeights)]] <- matrix(nrow = lastRow,ncol = lastCol,rnorm(lastCol*lastRow))
initialWeights <<- initialWeights
}
training <- function(x,y,learningRate,epochs=100){
initialweights <- initialWeights
x <- as.matrix(cbind(1,x))
for (i in 1:epochs) {
samples <- sample(1:length(x[,1]),1)
compWeights <- list(x[samples,])
length(compWeights) <-length(initialweights)
for (j in 1:length(initialweights)) {
values <- as.numeric(compWeights[[j]]) %*% initialweights[[j]]
compWeights[[j+1]]<-activation(values)
}
error <- y[samples,] - compWeights[[length(compWeights)]]
gradient <- list(error* activationDerivate(compWeights[[length(compWeights)]]))
for (k in (length(compWeights)-1):2) {
length(gradient) <- length(gradient) + 1
gradient[[length(gradient)]] <- gradient[[length(gradient)-1]]%*%t(initialweights[[k]])*activationDerivate(compWeights[[k]])
}
gradientReverse <- list()
length(gradientReverse) <- length(gradient)
for (m in 1:length(gradient)) {
gradientReverse[[m]] <- gradient[[(length(gradient))-m+1]]
}
for (s in 1:length(initialweights)) {
layerWeight <- as.numeric(compWeights[[s]])
gradient <- gradientReverse[[s]]
initialweights[[s]]<- initialweights[[s]] + learningRate*(layerWeight%*%gradient)
}
}
print(initialweights)
fitWeights <<- initialweights
}
prediction <- function(x){
for (i in 1:length(fitWeights)) {
dotValues <- x%*%fitWeights[[i]]
x <- activation(dotValues)
}
return(x)
}
predictionMat <- function(x){
newX <- cbind(1,x)
predictValues <- apply(newX, 1,function(x)prediction(x))
print(predictValues)
}
library(MASS)
samplesSelect<-sample(c(1:length(Boston[,1])),length(Boston[,1])*0.7)
trainData<-Boston[samplesSelect,]
testData<-Boston[-samplesSelect,]
trainX<-trainData[-which(colnames(Boston)=="medv")]
trainY<-trainData[which(colnames(Boston)=="medv")]
testX<-testData[-which(colnames(Boston)=="medv")]
testY<-testData[which(colnames(Boston)=="medv")]
trainX<-apply(trainX,2,function(x)scale(x,center=T,scale=T))
trainX<-scale(trainY,center=T,scale=T)
testX<-apply(testX,2,function(x)scale(x,center=T,scale=T))
testY<-scale(testY,center=T,scale=T)
myNeuralNet(c(13,6,3,2,1),'tanh')
training(trainX,trainY,0.001,20000)
training <- function(x,y,learningRate,epochs=100){
initialweights <- initialWeights
x <- as.matrix(cbind(1,x))
for (i in 1:epochs) {
samples <- sample(1:length(x[,1]),1)
compWeights <- list(x[samples,])
length(compWeights) <-length(initialweights)
compWeights <<- compWeights
for (j in 1:length(initialweights)) {
values <- compWeights[[j]] %*% initialweights[[j]]
compWeights[[j+1]]<-activation(values)
}
error <- y[samples,] - compWeights[[length(compWeights)]]
gradient <- list(error* activationDerivate(compWeights[[length(compWeights)]]))
for (k in (length(compWeights)-1):2) {
length(gradient) <- length(gradient) + 1
gradient[[length(gradient)]] <- gradient[[length(gradient)-1]]%*%t(initialweights[[k]])*activationDerivate(compWeights[[k]])
}
gradientReverse <- list()
length(gradientReverse) <- length(gradient)
for (m in 1:length(gradient)) {
gradientReverse[[m]] <- gradient[[(length(gradient))-m+1]]
}
for (s in 1:length(initialweights)) {
layerWeight <- as.numeric(compWeights[[s]])
gradient <- gradientReverse[[s]]
initialweights[[s]]<- initialweights[[s]] + learningRate*(layerWeight%*%gradient)
}
}
print(initialweights)
fitWeights <<- initialweights
}
myNeuralNet(c(13,6,3,2,1),'tanh')
training(trainX,trainY,0.001,20000)
View(compWeights)
View(initialWeights)
